{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pediatric-runner",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Demonstrates how to split datasets with CsvExampleGen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install tfx\n",
    "!pip install tensorflow-model-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import csv\n",
    "import os, pwd\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.components import (\n",
    "    CsvExampleGen,\n",
    "    FileBasedExampleGen,\n",
    "    ImportExampleGen\n",
    ")\n",
    "from tfx.proto import example_gen_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-university",
   "metadata": {},
   "source": [
    "## Errors when the code below\n",
    "If you receive an error `RuntimeError: Files in same split /Users/{usr}/Github/building-machine-learning-pipelines/data/* have different header.`, then execute the following command in the directory specified in the error.\n",
    "\n",
    "`ls -la`\n",
    "\n",
    "This will show that there is a hidden directory there. You must delete that checkpoint folder for this code to work. TFX will read all files/folders and generate an error is all the files in the directory are not CSV.\n",
    "\n",
    "```\n",
    "drwxr-xr-x   4       staff       128 Feb 20 08:25 .\n",
    "drwxr-xr-x  24 user  staff       768 Feb  8 21:55 ..\n",
    "drwxr-xr-x   2 user  staff        64 Feb 20 08:25 .ipynb_checkpoints\n",
    "-rw-r--r--@  1 user  staff  78956235 Feb 19 22:24 consumer_complaints_with_narrative.csv\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-nicholas",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "Splitting the input datasset into multiple **TFRecord** files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "print(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pwd.getpwuid(os.getuid()).pw_dir\n",
    "#base_dir = os.getcwd()\n",
    "data_dir_str = 'Github/building-machine-learning-pipelines/data'\n",
    "data_dir = os.path.join(base_dir, data_dir_str)\n",
    "original_data_file = os.path.join(data_dir, 'consumer_complaints_with_narrative.csv')\n",
    "\n",
    "\n",
    "# new_data_path = 'chap3_4_data_preparation_output'\n",
    "# try:\n",
    "#     os.mkdir(os.path.join(data_dir, new_data_path))\n",
    "# except:\n",
    "#     print(new_data_path + ' already exists.')\n",
    "    \n",
    "output = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=6),\n",
    "        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2),\n",
    "        example_gen_pb2.SplitConfig.Split(name='test', hash_buckets=2)]))\n",
    "examples = external_input(data_dir)\n",
    "example_gen = CsvExampleGen(input=examples, output_config=output)\n",
    "    \n",
    "        \n",
    "context = InteractiveContext()\n",
    "context.run(example_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-christmas",
   "metadata": {},
   "source": [
    "## View the output of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artifact in example_gen.outputs['examples'].get():\n",
    "    print(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-violence",
   "metadata": {},
   "source": [
    "## Preserving the input split\n",
    "If you data set was already split, then you can preserve it by using the input_config. This assumes that you have a train, test and eval directories with csv files included in the data directory.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pwd.getpwuid(os.getuid()).pw_dir\n",
    "#base_dir = os.getcwd()\n",
    "data_dir_str = 'Github/building-machine-learning-pipelines/data'\n",
    "data_dir = os.path.join(base_dir, data_dir_str)\n",
    "original_data_file = os.path.join(data_dir, 'consumer_complaints_with_narrative.csv')\n",
    "\n",
    "\n",
    "# new_data_path = 'chap3_4_data_preparation_output'\n",
    "# try:\n",
    "#     os.mkdir(os.path.join(data_dir, new_data_path))\n",
    "# except:\n",
    "#     print(new_data_path + ' already exists.')\n",
    "    \n",
    "input = example_gen_pb2.Input(splits=[\n",
    "        example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n",
    "        example_gen_pb2.Input.Split(name='eval', pattern='eval/*'),\n",
    "        example_gen_pb2.Input.Split(name='test', pattern='test/*')])\n",
    "examples = external_input(data_dir)\n",
    "example_gen = CsvExampleGen(input=examples, input_config=output)\n",
    "    \n",
    "        \n",
    "context = InteractiveContext()\n",
    "context.run(example_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-substance",
   "metadata": {},
   "source": [
    "## Spanning a dataset\n",
    "This shows how to span a data set. \n",
    "\n",
    "If process creates a new file with the previous dataset plus the new records, then you can use this code to include the new dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.utils.dsl_utils import external_input\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.components import (\n",
    "    CsvExampleGen\n",
    ")\n",
    "from tfx.proto import example_gen_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(os.pardir, \"data\")\n",
    "\n",
    "input = example_gen_pb2.Input(splits=[example_gen_pb2.Input.Split(pattern='export-{SPAN}/*')])\n",
    "\n",
    "examples = external_input(os.path.join(base_dir,data_dir))\n",
    "example_gen = CsvExampleGen(innput=examples, input_config=input)\n",
    "\n",
    "context = InteractiveContext()\n",
    "context.run(example_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
